{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3e34a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Здесь взят отфильтрованный по протоколу TCP в Wireshark трафик, здесь записи взаимодействия ботнета и машины.\n",
    "Аномальные записи были помечены красным и черным в интерфейсе Wireshark, поэтому мне было просто отличить их от чистых.\n",
    "Сбор датасета Okey был произведен вручную: вырезались из исходного датасета чистые фрагменты и собирались в единый список,\n",
    "поэтому по таймингу есть пустые места. Используется информация из графы Info.'''\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b34367",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('B:\\Учёба\\диплом\\трафик\\Okay.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430472c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Понадобилось почистить от знаков препинания, чтоб токенизация прошла лучше.'''\n",
    "def reformat(text):\n",
    "    return re.sub(\"[,|<|>|=|\\|/|.]\",' ',text)\n",
    "text = [reformat(i) for i in df['Info']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22480a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# После очистки проведена токенизация с длиной строки в 20, после чего посмотели на более часто встречающиеся слова.\n",
    "tokenizer = Tokenizer(num_words = 20)\n",
    "tokenizer.fit_on_texts(text)\n",
    "# произведена замена слов на индексы и получилось следующее - некоторые записи не были распознаны\n",
    "sequences = tokenizer.texts_to_sequences(text)\n",
    "new_sequences = sequences[:len(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7e9c2969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Здесь не критично, что в массиве new_sequences есть пустые записи, \n",
    "# их не будем принимать за аномалии, а просто удалим.\n",
    "empt = [i for i,x in enumerate(new_sequences) if x ==[] ]\n",
    "new_text = np.array(text)\n",
    "new_a = np.delete(text, empt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "06a34a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(new_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "727d8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Дальше формируется обучающий датасет, с маленьким коэффициентом аномальности, от 4 до 8'''\n",
    "x_train = pad_sequences(sequences, maxlen = 15)\n",
    "df_normal = pd.DataFrame(new_a, columns = ['tokens'])\n",
    "df_normal['expect'] = [np.sum(i)/len(i) for i in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1f2e54f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Датасет с аномальными данными собирался по томуже принципу, что и с чистыми, \n",
    "здесь повторяются те же действия, что и выше.'''\n",
    "df_anom = pd.read_csv('B:\\Учёба\\диплом\\трафик\\Anomaly.csv')\n",
    "text_anom = [reformat(i) for i in df_anom['Info']]\n",
    "\n",
    "tokenizer.fit_on_texts(text_anom)\n",
    "\n",
    "sequences_anom = tokenizer.texts_to_sequences(text_anom)\n",
    "\n",
    "empt_anom = [i for i,x in enumerate(sequences_anom) if x ==[] ]\n",
    "\n",
    "new_text = np.array(text_anom)\n",
    "new_anom = np.delete(text_anom, empt_anom)\n",
    "\n",
    "sequences_anom = tokenizer.texts_to_sequences(new_anom)\n",
    "\n",
    "df_anomaly = pd.DataFrame(new_anom, columns = ['tokens'])\n",
    "df_anomaly['expect'] = [np.sum(i)/len(i)+15 for i in sequences_anom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d1757754",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''После всех преобразований сформирован единый датасет и добавлено время.'''\n",
    "new_df = pd.concat([df_normal,df_anomaly])\n",
    "\n",
    "df_timeseries = df['Time'].iloc[:len(new_df)]\n",
    "\n",
    "new_df['time'] = df_timeseries\n",
    "\n",
    "new_df.to_csv('B:\\Учёба\\диплом\\трафик\\For_lstm.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
